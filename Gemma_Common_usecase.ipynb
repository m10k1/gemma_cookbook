{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMq5uIqYDDaM3DVgPtnd4p+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/m10k1/gemma_cookbook/blob/main/Gemma_Common_usecase.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gemma 一般的なユースケース\n",
        "\n",
        "このノートブックではGemmaが解決できる基本的なタスクについてのデモです。\n",
        "\n"
      ],
      "metadata": {
        "id": "OXMf12YicR3K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xxGLEiW4bfb-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U keras keras-nlp"
      ],
      "metadata": {
        "id": "7gUD2Gd2c4g-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d638e7f6-88f8-4ba4-8bf3-1921dd12f908"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m705.1/705.1 kB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploring prompting capabilities\n",
        "\n",
        "### Gemma\n",
        "\n",
        "#### Gemmaについて\n",
        "\n",
        "Gemmaは、Googleが提供する軽量で最先端のオープンモデルのファミリーで、Geminiモデルの作成に使用されたのと同じ研究と技術に基づいて構築されています。 Gemmaはテキスト対テキスト、デコーダのみの大規模言語モデルであり、オープンウェイト、事前学習済みバリアント、インストラクションチューニング済みバリアントがあり、英語で利用可能です。 Gemmaモデルは、質問応答、要約、推論を含む様々なテキスト生成タスクに適しています。 比較的小さなサイズであるため、ラップトップやデスクトップ、独自のクラウドインフラストラクチャなど、リソースが限られた環境でも導入が可能で、最先端のAIモデルへのアクセスを民主化し、すべての人のイノベーションを促進します。\n",
        "\n",
        "#### プロンプトフォーマット\n",
        "\n",
        "\n",
        "インストラクション・チューニング（IT）モデルは、学習時と推論時の両方において、すべてのインストラクション・チューニング例に余分な情報を注釈する特定のフォーマッタを使って学習される。 フォーマッターには2つの目的がある：\n",
        "\n",
        "*　システム、ユーザー、アシスタントの役割など、会話における役割を示す。\n",
        "* 特にマルチターンの会話において。\n",
        "\n",
        "以下では、Gemma が使用するコントロールトークンとその使用例を示す。 コントロール・トークンは、私たちのトークナイザで予約されており、私たちのトークナイザに固有であることに注意してください。\n",
        "\n",
        "* ユーザーターンを示すトークン：user\n",
        "* モデルのターンを示すトークン： model\n",
        "* 対話ターンの開始を示すトークン： \\<start_of_turn>\n",
        "* 対話ターンの終了を示すトークン： \\<end_of_turn>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "I9-EFNDxkMqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import keras_nlp\n",
        "from IPython.display import display, Markdown, Latex\n",
        "\n",
        "os.environ[\"KAGGLE_USERNAME\"] = userdata.get(\"KAGGLE_USERNAME\")\n",
        "os.environ[\"KAGGLE_KEY\"] = userdata.get(\"KAGGLE_KEY\")\n"
      ],
      "metadata": {
        "id": "ZuMMFHrslXHO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Let's load Gemma using Keras\n",
        "gemma_model_id = \"gemma_1.1_instruct_2b_en\"\n",
        "gemma = keras_nlp.models.GemmaCausalLM.from_preset(gemma_model_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-67uYir4lpOY",
        "outputId": "65d7e908-1fbe-4251-bf6d-06b099bebf3d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gemma/keras/gemma_1.1_instruct_2b_en/4/download/config.json...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 785/785 [00:00<00:00, 782kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gemma/keras/gemma_1.1_instruct_2b_en/4/download/model.weights.h5...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.67G/4.67G [00:33<00:00, 151MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gemma/keras/gemma_1.1_instruct_2b_en/4/download/tokenizer.json...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 591/591 [00:00<00:00, 1.33MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gemma/keras/gemma_1.1_instruct_2b_en/4/download/assets/tokenizer/vocabulary.spm...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.04M/4.04M [00:00<00:00, 103MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"user\n",
        "\n",
        "Text:\n",
        "Tech giant Google, originally a Stanford research project by Larry Page and Sergey Brin,\n",
        "has become synonymous with searching the web. Their complex algorithms prioritize relevant\n",
        "results, making Google the go-to platform for information seekers. Beyond search,\n",
        "Google offers email (Gmail), document creation tools (Google Docs), mapping (Google Maps),\n",
        "the Android operating system for smartphones, and the Chrome web browser.\n",
        "A leader in AI, Google ventures into self-driving cars and quantum computing.\n",
        "Their influence is so vast, \"to google\" has become a verb for searching online.\n",
        "\n",
        "Could you summarize the text in a few words?\n",
        "model\"\"\"\n",
        "\n",
        "response = gemma.generate(prompt, max_length=512)\n",
        "display(Markdown(response[len(prompt) :]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "R6Q5rDJ70aKJ",
        "outputId": "ff0812d6-30dc-4c57-b3ba-cdb415199691"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\nGoogle is a technology giant known for its search engine, email, documents, maps, and more."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"user\n",
        "Think and write your step-by-step reasoning before responding.\n",
        "\n",
        "Explain why the sky is blue.\n",
        "model\"\"\"\n",
        "\n",
        "response = gemma.generate(prompt, max_length=1024)\n",
        "display(Markdown(response[len(prompt) :]))"
      ],
      "metadata": {
        "id": "F-YQbKzUl8vc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "133bfafd-2619-4eb3-935c-43b03e28312b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n\n**Step 1: Light enters the atmosphere.**\n- Sunlight, composed of various wavelengths of colors, enters the Earth's atmosphere from the sun.\n\n**Step 2: Rayleigh Scattering.**\n- When sunlight interacts with molecules in the atmosphere, certain wavelengths of light are scattered more efficiently than others.\n- Blue light, with its shorter wavelengths, is scattered more extensively than other colors.\n\n**Step 3: Blue light reaches our eyes.**\n- As sunlight enters our eyes, the blue light is scattered away from our view, leaving behind the other colors that we perceive as colors.\n\n**Step 4: Perception of blue sky.**\n- The scattered blue light reaches our eyes and stimulates the cones in our retina, which interpret it as blue.\n- This is why we experience the sky as blue during clear weather conditions."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"user\n",
        "Classify the text into neutral, negative, or positive.\n",
        "Generate only the class, nothing else.\n",
        "Text: Text: I think the food was awesome.\n",
        "model\n",
        "Sentiment:\"\"\"\n",
        "\n",
        "response = gemma.generate(prompt, max_length=64)\n",
        "print(\"Sentiment: \" + response[len(prompt) :])\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpYsyasS1Gcg",
        "outputId": "cdacb841-e09e-4801-c92b-38e7cbcbf3b4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment:  Positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"Lost in a worn Berlin travel guide, Ania underlined Warsaw and London.\n",
        "Two cities, two dreams: a bustling career in finance and\n",
        "a cozy bookshop tucked away in a cobbled London street.\"\"\"\n",
        "\n",
        "prompt = f\"\"\"user\n",
        "Your task is to find all the city names in the given text.\n",
        "Your response is an array of the model names in the format [\\\"city_name\\\"].\n",
        "If you don't find model names in the abstract or you are not sure, return [\\\"\\\"]\n",
        "\n",
        "Text:\n",
        "{text}\n",
        "model\"\"\"\n",
        "\n",
        "response = gemma.generate(prompt, max_length=512)\n",
        "print(\"Extracted cities: \" + response[len(prompt) :])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2Y4GQop1Nez",
        "outputId": "5f39f0ec-1c86-4ad0-f219-15c969fff942"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted cities:  names: [\"\"]\n",
            "\n",
            "Answer:\n",
            "[\"Berlin\", \"Warsaw\", \"London\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import textwrap\n",
        "\n",
        "\n",
        "def format_messages(messages):\n",
        "    fmt_messages = [\n",
        "        f\"{author}\\n{text}\" for author, text in messages\n",
        "    ]\n",
        "    fmt_messages.append(\"model\")\n",
        "    return \"\\n\".join(fmt_messages)\n",
        "\n",
        "\n",
        "history = [\n",
        "    (\n",
        "        \"user\",\n",
        "        \"You're a helpful assistant. Answer user's question in a friendly and helpful way. \"\n",
        "        \"Please introduce yourself first.\",\n",
        "    )\n",
        "]\n",
        "\n",
        "print(\"\\n*** YOU'RE ENTERING CHAT MODE. TYPE 'EXIT' TO FINISH THE CHAT ***\\n\")\n",
        "while True:\n",
        "    prompt = format_messages(history)\n",
        "    response = gemma.generate(prompt, max_length=512)\n",
        "    response = response[len(prompt) :]\n",
        "\n",
        "    history.append((\"model\", response))\n",
        "    print(textwrap.fill(f\"Gemma: {response} \\n\", 120))\n",
        "\n",
        "    question = input(\"User: \")\n",
        "    if question == \"EXIT\":\n",
        "        break\n",
        "\n",
        "    print()\n",
        "    history.append((\"user\", question))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZfmzbUN1Ry_",
        "outputId": "86aa1afc-ea4c-455c-f08e-08c6d0e05190"
      },
      "execution_count": 9,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "*** YOU'RE ENTERING CHAT MODE. TYPE 'EXIT' TO FINISH THE CHAT ***\n",
            "\n",
            "Gemma:   Hi there! I'm your friendly AI assistant here to help you with any questions or tasks you may have. I'm here to\n",
            "assist you with information, provide solutions, and make your experience more efficient. Let's get started!\n",
            "User: Hello\n",
            "\n",
            "Gemma:   Hello! I'm glad to be of assistance. I'm excited to learn more about how I can help you today. How can I assist\n",
            "you?\n",
            "User: EXIT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"user\n",
        "Generate a Python function that multiplies two numbers\n",
        "model\"\"\"\n",
        "\n",
        "response = gemma.generate(prompt, max_length=1024)\n",
        "display(Markdown(response[len(prompt) :]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        },
        "id": "rtlHvBSh1V9_",
        "outputId": "e9e3b1b8-8c09-4246-d98f-f1885e8cee6f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n```python\ndef multiply(num1, num2):\n  \"\"\"\n  This function multiplies two numbers.\n\n  Args:\n    num1: The first number.\n    num2: The second number.\n\n  Returns:\n    The product of num1 and num2.\n  \"\"\"\n\n  product = num1 * num2\n  return product\n```\n\n**Usage:**\n\n```python\n# Multiply 5 and 10\nproduct = multiply(5, 10)\n\n# Print the product\nprint(product)  # Output: 50\n```\n\n**Explanation:**\n\n* The function takes two parameters, `num1` and `num2`.\n* Inside the function, we use the multiplication operator (`*`) to calculate the product of `num1` and `num2`.\n* The product is stored in the `product` variable.\n* The function returns the calculated product.\n\n**Additional Notes:**\n\n* This function only works for positive numbers. For negative numbers, a `ZeroDivisionError` will be raised.\n* The function is a simple implementation of multiplication and does not handle any errors or special cases. For more complex multiplication scenarios, additional logic may be required."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t75SayxT1W1b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}